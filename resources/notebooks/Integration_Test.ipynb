{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28017 RunJar\n",
      "29109 Jps\n",
      "26616 DataNode\n",
      "27081 ResourceManager\n",
      "26859 SecondaryNameNode\n",
      "28844 SparkSubmit\n",
      "26412 NameNode\n",
      "27405 NodeManager\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example.csv\n"
     ]
    }
   ],
   "source": [
    "%%file example.csv\n",
    "id,firstname,lastname,email,profession\n",
    "100,Allyce,Dom,Allyce.Dom@gmail.com,firefighter\n",
    "101,Gertrud,Poppy,Gertrud.Poppy@gmail.com,worker\n",
    "102,Jacquetta,Sigfrid,Jacquetta.Sigfrid@gmail.com,worker\n",
    "103,Desirae,Mich,Desirae.Mich@gmail.com,firefighter\n",
    "104,Kalina,Edee,Kalina.Edee@gmail.com,firefighter\n",
    "105,Bibby,Raffo,Bibby.Raffo@gmail.com,developer\n",
    "106,Maisey,Erlandson,Maisey.Erlandson@gmail.com,firefighter\n",
    "107,Adriana,Deegan,Adriana.Deegan@gmail.com,firefighter\n",
    "108,Eve,Durante,Eve.Durante@gmail.com,developer\n",
    "109,Clarice,Robertson,Clarice.Robertson@gmail.com,firefighter\n",
    "110,Patricia,Alexandr,Patricia.Alexandr@gmail.com,firefighter\n",
    "111,Eolanda,Ursulette,Eolanda.Ursulette@gmail.com,developer\n",
    "112,Nannie,Ilka,Nannie.Ilka@gmail.com,doctor\n",
    "113,Dagmar,Mauer,Dagmar.Mauer@gmail.com,police officer\n",
    "114,Shaylyn,Mich,Shaylyn.Mich@gmail.com,developer\n",
    "115,Gilda,Daveta,Gilda.Daveta@gmail.com,worker\n",
    "116,Carree,Faust,Carree.Faust@gmail.com,police officer\n",
    "117,Collen,Kaete,Collen.Kaete@gmail.com,developer\n",
    "118,Gloria,Bendick,Gloria.Bendick@gmail.com,worker\n",
    "119,Kary,Eachern,Kary.Eachern@gmail.com,developer\n",
    "120,Margette,Therine,Margette.Therine@gmail.com,doctor\n",
    "121,Nataline,Kosey,Nataline.Kosey@gmail.com,developer\n",
    "122,Romona,Ludewig,Romona.Ludewig@gmail.com,firefighter\n",
    "123,Eadie,Tyson,Eadie.Tyson@gmail.com,police officer\n",
    "124,Jan,Geffner,Jan.Geffner@gmail.com,worker\n",
    "125,Corina,Tomasina,Corina.Tomasina@gmail.com,police officer\n",
    "126,Ellette,Bearnard,Ellette.Bearnard@gmail.com,firefighter\n",
    "127,Marinna,Peg,Marinna.Peg@gmail.com,worker\n",
    "128,Brana,Burnside,Brana.Burnside@gmail.com,worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/vagrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-25 06:27:23,391 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put example.csv /user/vagrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 vagrant supergroup       1574 2020-04-25 06:27 /user/vagrant/example.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/vagrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_hive.hql\n"
     ]
    }
   ],
   "source": [
    "%%file test_hive.hql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS test_table\n",
    " (col1 int COMMENT 'Integer Column',\n",
    " col2 string COMMENT 'String Column')\n",
    " COMMENT 'This is test table'\n",
    " ROW FORMAT DELIMITED\n",
    " FIELDS TERMINATED BY ','\n",
    " STORED AS TEXTFILE;\n",
    "    \n",
    "INSERT INTO test_table VALUES(1,'testing');\n",
    "\n",
    "SELECT * FROM test_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Hive Session ID = 3d6a1453-55f9-4efb-93d0-98b479855744\n",
      "\n",
      "Logging initialized using configuration in jar:file:/opt/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "Hive Session ID = 58f11f53-0938-44dc-9010-b371a553e228\n",
      "OK\n",
      "Time taken: 1.27 seconds\n",
      "Query ID = vagrant_20200425060752_e639ab87-2681-4878-9c0e-a504d3cb6cb7\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1587793898293_0001, Tracking URL = http://vagrant.vm:8088/proxy/application_1587793898293_0001/\n",
      "Kill Command = /opt/hadoop/bin/mapred job  -kill job_1587793898293_0001\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2020-04-25 06:08:03,261 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-04-25 06:08:07,452 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.35 sec\n",
      "2020-04-25 06:08:12,669 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.45 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 450 msec\n",
      "Ended Job = job_1587793898293_0001\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://10.0.2.15:9000/user/hive/warehouse/test_table/.hive-staging_hive_2020-04-25_06-07-52_688_607488361951007403-1/-ext-10000\n",
      "Loading data to table default.test_table\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.45 sec   HDFS Read: 15497 HDFS Write: 247 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 450 msec\n",
      "OK\n",
      "Time taken: 21.628 seconds\n",
      "OK\n",
      "1\ttesting\n",
      "Time taken: 0.317 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f ./test_hive.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: hive@metastore'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext sql\n",
    "%sql mysql://hive:3MX6fDLN2AQm23bD@localhost/metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql://hive:***@localhost/metastore\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>TBL_ID</th>\n",
       "        <th>CREATE_TIME</th>\n",
       "        <th>OWNER</th>\n",
       "        <th>TBL_NAME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "        <td>1587794872</td>\n",
       "        <td>vagrant</td>\n",
       "        <td>test_table</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 1587794872, 'vagrant', 'test_table')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select TBL_ID, CREATE_TIME, OWNER, TBL_NAME\n",
    "from TBLS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://localhost:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "| default|test_table|      false|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|col1|   col2|\n",
      "+----+-------+\n",
      "|   1|testing|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(\"select * from test_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\n",
      "    link/ether 08:00:27:bc:dc:a4 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0\n",
      "       valid_lft 84345sec preferred_lft 84345sec\n",
      "    inet6 fe80::a00:27ff:febc:dca4/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n"
     ]
    }
   ],
   "source": [
    "!ip addr show dev eth0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksession = SparkSession.builder.appName(\"example-pyspark-read-and-write\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (sparksession\n",
    "\t.read\n",
    "\t.format(\"csv\")\n",
    "\t.option(\"header\", \"true\")\n",
    "\t.load(\"hdfs://10.0.2.15:9000/user/vagrant/example.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+--------------------+--------------+\n",
      "| id|firstname| lastname|               email|    profession|\n",
      "+---+---------+---------+--------------------+--------------+\n",
      "|100|   Allyce|      Dom|Allyce.Dom@gmail.com|   firefighter|\n",
      "|101|  Gertrud|    Poppy|Gertrud.Poppy@gma...|        worker|\n",
      "|102|Jacquetta|  Sigfrid|Jacquetta.Sigfrid...|        worker|\n",
      "|103|  Desirae|     Mich|Desirae.Mich@gmai...|   firefighter|\n",
      "|104|   Kalina|     Edee|Kalina.Edee@gmail...|   firefighter|\n",
      "|105|    Bibby|    Raffo|Bibby.Raffo@gmail...|     developer|\n",
      "|106|   Maisey|Erlandson|Maisey.Erlandson@...|   firefighter|\n",
      "|107|  Adriana|   Deegan|Adriana.Deegan@gm...|   firefighter|\n",
      "|108|      Eve|  Durante|Eve.Durante@gmail...|     developer|\n",
      "|109|  Clarice|Robertson|Clarice.Robertson...|   firefighter|\n",
      "|110| Patricia| Alexandr|Patricia.Alexandr...|   firefighter|\n",
      "|111|  Eolanda|Ursulette|Eolanda.Ursulette...|     developer|\n",
      "|112|   Nannie|     Ilka|Nannie.Ilka@gmail...|        doctor|\n",
      "|113|   Dagmar|    Mauer|Dagmar.Mauer@gmai...|police officer|\n",
      "|114|  Shaylyn|     Mich|Shaylyn.Mich@gmai...|     developer|\n",
      "|115|    Gilda|   Daveta|Gilda.Daveta@gmai...|        worker|\n",
      "|116|   Carree|    Faust|Carree.Faust@gmai...|police officer|\n",
      "|117|   Collen|    Kaete|Collen.Kaete@gmai...|     developer|\n",
      "|118|   Gloria|  Bendick|Gloria.Bendick@gm...|        worker|\n",
      "|119|     Kary|  Eachern|Kary.Eachern@gmai...|     developer|\n",
      "+---+---------+---------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
